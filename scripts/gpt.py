import numpy as np
import json
import re
import string
from IPython.display import display, HTML

import tensorflow as tf
from tensorflow.keras import layers, models, losses, callbacks

# # Loading Dataset

VOCAB_SIZE = 10000
MAX_LEN = 80
EMBEDDING_DIM = 256
KEY_DIM = 256
N_HEADS = 2
FEED_FORWARD_DIM = 256
VALIDATION_SPLIT = 0.2
SEED = 42
LOAD_MODEL = False
BATCH_SIZE = 32
EPOCHS = 20

with open("../data/winemag-data-130k-v2.json") as json_data:
    wine_data = json.load(json_data)

filtered_data = ["wine review : " + x["country"] + " : " + x["province"] + " : " +
                     x["variety"] + " : " + x["description"]
                 for x in wine_data
                 if x["country"] is not None
                and x["province"] is not None
                and x["variety"] is not None
                and x["description"] is not None]

# Count the recipes
n_wines = len(filtered_data)
print(f"\n{n_wines} recipes loaded!\n")
print(f'\nExample of filtered data:\n{filtered_data[25]}\n')

# # Tokenize Data

def pad_punctuation(s):
    s = re.sub(f"([{string.punctuation}, '\n'])", r" \1 ", s)
    s = re.sub(" +", " ", s)
    return s

text_data = [pad_punctuation(x) for x in filtered_data]
print(f'\nExample of tokenized data:\n{text_data[25]}\n')

text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(BATCH_SIZE).shuffle(1000)

vectorize_layer = layers.TextVectorization(standardize='lower',
                                           max_tokens=VOCAB_SIZE,
                                           output_mode='int',
                                           output_sequence_length=MAX_LEN + 1)

vectorize_layer.adapt(text_ds)
vocab = vectorize_layer.get_vocabulary()

for i, word in enumerate(vocab[:10]):
    print(f'{i}: {word}')

example_tokenised = vectorize_layer(text_data[25])
print(f'Example tokens:\n{example_tokenised.numpy()}')

# # Create the Training Set

def prepare_inputs(text):
    texts = tf.expand_dims(text, -1)
    tokenized_sentences = vectorize_layer(texts)
    x = tokenized_sentences[:, :-1]
    y = tokenized_sentences[:, 1:]
    return x, y

train_ds = text_ds.map(prepare_inputs)

example_input_output = train_ds.take(1).get_single_element()
print(example_input_output[0][0])
print(example_input_output[1][0])

# # Create the Causal Attention Mask Function

def causal_attention_mask(batch_size, n_dest, n_src, dtype):
    i = tf.range(n_dest)[:, None]
    j = tf.range(n_src)
    m = i >=j - n_src * n_dest
    mask = tf.cast(m, dtype)
    mask = tf.reshape(mask, [1, n_dest, n_src])
    mult = tf.concat([tf.expand_dims(batch_size, -1),
                      tf.constant([1, 1], dtype=tf.int32)], 0)
    return tf.tile(mask, mult)

np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])

# # Create a Transformer Block Layer

class TransformerBlock(layers.Layer):
    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):
        super(TransformerBlock, self).__init__()
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.dropout_rate = dropout_rate
        self.attn = layers.MultiHeadAttention(num_heads, key_dim, output_shape=embed_dim)
        self.dropout_1 = layers.Dropout(self.dropout_rate)
        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)
        self.ffn_1 = layers.Dense(self.ff_dim, activation="relu")
        self.ffn_2 = layers.Dense(self.embed_dim)
        self.dropout_2 = layers.Dropout(self.dropout_rate)
        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)

    def call(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size = input_shape[0]
        seq_len = input_shape[1]
        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)
        attention_output, attention_scores = self.attn(
                inputs,
                inputs,
                attention_mask=causal_mask,
                return_attention_scores=True)
        attention_output = self.dropout_1(attention_output)
        out1 = self.ln_1(inputs + attention_output)
        ffn_1 = self.ffn_1(out1)
        ffn_2 = self.ffn_2(ffn_1)
        ffn_output = self.dropout_2(ffn_2)
        return (self.ln_2(out1 + ffn_output), attention_scores)

    def get_config(self):
        config = super().get_config()
        config.update({"key_dim": self.key_dim,
                       "embed_dim": self.embed_dim,
                       "num_heads": self.num_heads,
                       "ff_dim": self.ff_dim,
                       "dropout_rate": self.dropout_rate})
        return config

# # Create the Token and Position Embedding

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, max_len, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.max_len = max_len
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

    def get_config(self):
        config = super().get_config()
        config.update({"max_len": self.max_len,
                       "vocab_size": self.vocab_size,
                       "embed_dim": self.embed_dim})
        return config

# # Build the Transformer Model

inputs = layers.Input(shape=(None, ), dtype=tf.int32)
x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)
x, attention_scores = TransformerBlock(N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM)(x)
outputs = layers.Dense(VOCAB_SIZE, activation="softmax")(x)
gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])
gpt.compile("adam", loss=[losses.SparseCategoricalCrossentropy(), None])

gpt.summary()

if LOAD_MODEL:
    # model.load_weights('./models/model')
    gpt = models.load_model("./models/gpt", compile=True)

# # Train the Transformer

class TextGenerator(callbacks.Callback):
    def __init__(self, index_to_word, top_k=10):
        self.index_to_word = index_to_word
        self.word_to_index = {word: index for index, word in enumerate(index_to_word)}

    def sample_from(self, probs, temperature):
        probs = probs ** (1 / temperature)
        probs = probs / np.sum(probs)
        return np.random.choice(len(probs), p=probs), probs

    def generate(self, start_prompt, max_tokens, temperature):
        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]
        sample_token = None
        info = []
        while len(start_tokens) < max_tokens and sample_token != 0:
            x = np.array([start_tokens])
            y, att = self.model.predict(x, verbose=0)
            sample_token, probs = self.sample_from(y[0][-1], temperature)
            info.append({"prompt": start_prompt, "word_probs": probs, "atts": att[0, :, -1, :]})
            start_tokens.append(sample_token)
            start_prompt = start_prompt + " " + self.index_to_word[sample_token]
        print(f'\nGenerate text:\n{start_prompt}\n')
        return info

    def on_epoch_end(self, epoch, logs=None):
        self.generate("wine review", max_tokens=80, temperature=1.0)

model_checkpoint_callback = callbacks.ModelCheckpoint(
        filepath="./checkpoint/gpt-checkpoint.weights.h5",
        save_weights_only=True,
        save_freq="epoch",
        verbose=0)

tensorboard_callback = callbacks.TensorBoard(log_dir='./logs')

text_generator = TextGenerator(vocab)

gpt.fit(train_ds, epochs=EPOCHS, callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator])

gpt.save('./models/gpt.keras')

# # Generate Text using the Transformer

def print_probs(info, vocab, top_k=5):
    for i in info:
        highlighted_text = []
        for word, att_score in zip(i["prompt"].split(), np.mean(i['atts'], axis=0)):
            highlighted_text.append(
                    '<span style="background-color:rgba(135,206,250,'
                    + str(att_score / max(np.mean(i["atts"], axis=0)))
                    + ');">'
                    + word
                    + "</span>")
        highlighted_text = " ".join(highlighted_text)
        display(HTML(highlighted_text))

        word_probs = i['word_probs']
        p_sorted = np.sort(word_probs)[::-1][:top_k]
        i_sorted = np.argsort(word_probs)[::-1][:top_k]
        for p, i in zip(p_sorted, i_sorted):
            print(f'{vocab[i]}:    {np.round(100 * p, 2)}%')
        print('------\n')

info = text_generator.generate("wine review : us", max_tokens=80, temperature=1.0)
info = text_generator.generate("wine review : italy", max_tokens=80, temperature=0.5)
info = text_generator.generate("wine review : germany", max_tokens=80, temperature=0.5)

print_probs(info, vocab)
